import os
import base64
import re
import matplotlib.pyplot as plt
from openai import OpenAI
import shutil

import base64
import sys
import cv2
import time
import shutil
import base64
from enum import Enum
from typing import Optional, List
from openai import OpenAI
# é€šè¿‡ pip install volcengine-python-sdk[ark] å®‰è£…æ–¹èˆŸSDK
from volcenginesdkarkruntime._exceptions import ArkAPIError
from volcenginesdkarkruntime import Ark
import ffmpeg
import io


import sys
import cv2
import time
import ffmpeg
import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")

# é…ç½® API å®¢æˆ·ç«¯
client = OpenAI(
    api_key="sk-814bb34a29cf45ee9a98d18b00b9eebf",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)


def trim_audio_ffmpeg(input_path, output_path, start_time, duration):
    """
    ä½¿ç”¨ ffmpeg-python æˆªå–è§†é¢‘
    :param input_path: åŸå§‹è§†é¢‘æ–‡ä»¶è·¯å¾„
    :param output_path: æˆªå–åè§†é¢‘æ–‡ä»¶è·¯å¾„
    :param start_time: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    :param duration: æˆªå–æ—¶é•¿ï¼ˆç§’ï¼‰
    """

    ffmpeg.input(input_path, ss=start_time, t=duration).output(output_path).run(overwrite_output=True, quiet=True)
    print(f"âœ… è§†é¢‘å·²æˆåŠŸæˆªå–å¹¶ä¿å­˜è‡³ {output_path}")

def encode_audio(audio_path):
    """å°†è§†é¢‘æ–‡ä»¶ç¼–ç ä¸º base64 å­—ç¬¦ä¸²"""
    with open(audio_path, "rb") as audio_file:
        return base64.b64encode(audio_file.read()).decode("utf-8")


def parse_emotion_percentages(text):
    """
    ä»æ–‡æœ¬ä¸­è§£ææƒ…æ„Ÿå æ¯”æ•°æ®
    ç¤ºä¾‹è¾“å…¥ï¼š"åˆ†æç»“æœæ˜¾ç¤ºï¼šå–œæ‚¦ï¼ˆ30%ï¼‰ã€æ‚²ä¼¤ï¼ˆ50%ï¼‰ã€ä¸­ç«‹ï¼ˆ20%ï¼‰"
    è¿”å›ï¼š{"å–œæ‚¦": 0.3, "æ‚²ä¼¤": 0.5, "ä¸­ç«‹": 0.2}
    """
    pattern = r'([\u4e00-\u9fa5]+)[\sï¼ˆ(]*(\d+)%[ï¼‰)]*'
    matches = re.findall(pattern, text)

    emotion_dict = {}
    total = 0
    for emotion, percent in matches:
        value = float(percent) / 100
        emotion_dict[emotion] = value
        total += value

    # æ ‡å‡†åŒ–å¤„ç†ï¼ˆç¡®ä¿æ€»å’Œä¸º1ï¼‰
    if total > 0:
        return {k: v / total for k, v in emotion_dict.items()}
    return {}


def plot_emotion_pie(emotion_data):
    """ç»˜åˆ¶æƒ…æ„Ÿåˆ†å¸ƒé¥¼çŠ¶å›¾å¹¶ä¿å­˜åˆ°ä¸è„šæœ¬åŒç›®å½•ä¸‹çš„ images æ–‡ä»¶å¤¹"""

    if not emotion_data:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æƒ…æ„Ÿæ•°æ®å¯ä¾›å¯è§†åŒ–")
        return


    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    save_dir = os.path.join(script_dir, "images")

    # åˆ›å»º images ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    labels = [f"{k} ({v * 100:.1f}%)" for k, v in emotion_data.items()]
    sizes = list(emotion_data.values())
    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0']

    plt.figure(figsize=(10, 7))
    plt.rcParams['font.sans-serif'] = ['SimHei']  # è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

    patches, texts, autotexts = plt.pie(
        sizes,
        labels=labels,
        colors=colors,
        autopct='%1.1f%%',
        startangle=90,
        pctdistance=0.85,
        wedgeprops={'linewidth': 1, 'edgecolor': 'white'}
    )

    for text in texts + autotexts:

        text.set_fontsize(40)
        text.set_color('#333333')
#å»é™¤ç¯çŠ¶æ•ˆæœ
#    centre_circle = plt.Circle((0, 0), 0.70, fc='white')
#    plt.gca().add_artist(centre_circle)

    plt.title('è§†é¢‘æƒ…æ„Ÿåˆ†æç»“æœåˆ†å¸ƒ', fontsize=40, pad=10)
    plt.axis('equal')
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡åˆ° images æ–‡ä»¶å¤¹
    output_path = os.path.join(save_dir, f"emotion_pie_{int(time.time())}.png")
    plt.savefig(output_path, dpi=600, bbox_inches='tight')
    print(f"image_path:{output_path}")  # è¾“å‡ºå›¾ç‰‡è·¯å¾„åˆ°æ ‡å‡†è¾“å‡º


# ä¸»ç¨‹åºæµç¨‹
if __name__ == "__main__":

    plt.ioff()  # å…³é—­äº¤äº’å¼æ¨¡å¼

    # ========== å‚æ•°æ£€æŸ¥å¢å¼º ==========
    if len(sys.argv) != 2:
        sys.exit(1)

    input_audio_path = sys.argv[1]
    trimmed_audio_path = input_audio_path.replace(".mp4","_trimmed.mp4")  # æˆªå–åæ–‡ä»¶è·¯å¾„
    trim_audio_ffmpeg(input_audio_path, trimmed_audio_path, start_time=0, duration=3)

    # Step 2: ç¼–ç æˆªå–åçš„è§†é¢‘
    base64_audio = encode_audio(trimmed_audio_path)

    # Step 3: å‘é€åˆ†æè¯·æ±‚
    completion = client.chat.completions.create(
        model="qwen-omni-turbo",
        messages=[
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘æƒ…æ„Ÿåˆ†æåŠ©æ‰‹ï¼Œè¯·ç”¨æ˜ç¡®æ ¼å¼å›ç­”ï¼ŒåŒ…å«å…·ä½“ç™¾åˆ†æ¯”ã€‚"}],
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "video_url",
                        "video_url": {"url": f"data:;base64,{base64_audio}"},
                    },
                    {"type": "text",
                     "text": "åˆ†æè¿™æ®µè§†é¢‘çš„æƒ…æ„Ÿæ„æˆå¹¶æ€»ç»“è§†é¢‘å†…å®¹ç±»åˆ«ï¼Œæƒ…æ„Ÿæ„æˆè¯·ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼šæƒ…æ„Ÿåç§°ï¼ˆç™¾åˆ†æ¯”%ï¼‰ï¼Œè‡³å°‘ä¸‰ä¸ªåŠä»¥ä¸Šæ„Ÿæƒ…ï¼Œè§†é¢‘ç±»åˆ«ä»å¿«ä¹ï¼Œå°„å‡»ï¼Œä¼‘é—²ä¸‰ä¸ªæ ‡ç­¾ä¸­ä»»æ„é€‰ä¸¤ä¸ª"},
                ],
            },
        ],

#        modalities=["text"],
        stream=True,
        stream_options={"include_usage": True},
    )

    # Step 4: æ”¶é›†å®Œæ•´å“åº”
    full_text = ""
    for chunk in completion:
        if chunk.choices and chunk.choices[0].delta.content:
            chunk_content = chunk.choices[0].delta.content
            full_text += chunk_content

    # Step 5: è§£æå¹¶å¯è§†åŒ–
    emotion_data = parse_emotion_percentages(full_text)

    if emotion_data:

#            print("\nğŸ­ æ£€æµ‹åˆ°ä»¥ä¸‹æƒ…æ„Ÿåˆ†å¸ƒï¼š")
#            for emotion, percent in emotion_data.items():
#                print(f"- {emotion}: {percent * 100:.1f}%")

            plot_emotion_pie(emotion_data)
